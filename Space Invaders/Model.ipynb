{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.0.22, Python 3.7.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(tf.__version__)\\nprint(len(tf.config.list_physical_devices(\\'GPU\\'))>0)\\n\\n# Check GPU availability\\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\\'GPU\\')))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(tf.__version__)\n",
    "print(len(tf.config.list_physical_devices('GPU'))>0)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:470.0\n",
      "Episode:2 Score:125.0\n",
      "Episode:3 Score:225.0\n",
      "Episode:4 Score:80.0\n",
      "Episode:5 Score:110.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr = 'eps', value_max = 1., value_min = .1, value_test = .2, nb_steps = 10000)\n",
    "    memory = SequentialMemory(limit = 1000, window_length = 3)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy,\n",
    "                   enable_dueling_network = True, dueling_type = 'avg',\n",
    "                   nb_actions = actions, nb_steps_warmup = 1000\n",
    "                   )\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "\n",
    "model = build_model(height, width, channels, actions)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\miniconda3\\envs\\Space_Invaders\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  662/10000: episode: 1, duration: 78.649s, episode steps: 662, steps per second:   8, episode reward: 155.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.414 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 1155/10000: episode: 2, duration: 99.394s, episode steps: 493, steps per second:   5, episode reward: 110.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 2.400 [0.000, 5.000],  loss: 30.946419, mean_q: 9.522295, mean_eps: 0.903025\n",
      " 1874/10000: episode: 3, duration: 172.687s, episode steps: 719, steps per second:   4, episode reward: 180.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.441 [0.000, 5.000],  loss: 1.476381, mean_q: 8.079699, mean_eps: 0.863740\n",
      " 2832/10000: episode: 4, duration: 227.776s, episode steps: 958, steps per second:   4, episode reward: 185.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.633 [0.000, 5.000],  loss: 0.983374, mean_q: 8.706827, mean_eps: 0.788275\n",
      " 3564/10000: episode: 5, duration: 174.563s, episode steps: 732, steps per second:   4, episode reward: 110.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.534 [0.000, 5.000],  loss: 0.621435, mean_q: 6.509814, mean_eps: 0.712225\n",
      " 4253/10000: episode: 6, duration: 240.756s, episode steps: 689, steps per second:   3, episode reward: 120.000, mean reward:  0.174 [ 0.000, 30.000], mean action: 2.403 [0.000, 5.000],  loss: 0.619421, mean_q: 7.138775, mean_eps: 0.648280\n",
      " 4880/10000: episode: 7, duration: 234.029s, episode steps: 627, steps per second:   3, episode reward: 155.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.456 [0.000, 5.000],  loss: 0.458699, mean_q: 7.893101, mean_eps: 0.589060\n",
      " 5320/10000: episode: 8, duration: 165.228s, episode steps: 440, steps per second:   3, episode reward: 50.000, mean reward:  0.114 [ 0.000, 15.000], mean action: 2.320 [0.000, 5.000],  loss: 0.374315, mean_q: 7.124239, mean_eps: 0.541045\n",
      " 6014/10000: episode: 9, duration: 259.818s, episode steps: 694, steps per second:   3, episode reward: 110.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 2.414 [0.000, 5.000],  loss: 0.385377, mean_q: 6.920353, mean_eps: 0.490015\n",
      " 6697/10000: episode: 10, duration: 148.822s, episode steps: 683, steps per second:   5, episode reward: 180.000, mean reward:  0.264 [ 0.000, 30.000], mean action: 2.442 [0.000, 5.000],  loss: 0.485724, mean_q: 7.152892, mean_eps: 0.428050\n",
      " 7332/10000: episode: 11, duration: 98.413s, episode steps: 635, steps per second:   6, episode reward: 110.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 2.831 [0.000, 5.000],  loss: 0.369963, mean_q: 7.017329, mean_eps: 0.368740\n",
      " 7785/10000: episode: 12, duration: 67.878s, episode steps: 453, steps per second:   7, episode reward: 45.000, mean reward:  0.099 [ 0.000, 20.000], mean action: 2.993 [0.000, 5.000],  loss: 0.268884, mean_q: 7.033377, mean_eps: 0.319780\n",
      " 8539/10000: episode: 13, duration: 107.855s, episode steps: 754, steps per second:   7, episode reward: 140.000, mean reward:  0.186 [ 0.000, 30.000], mean action: 2.760 [0.000, 5.000],  loss: 0.307111, mean_q: 6.861408, mean_eps: 0.265465\n",
      " 9441/10000: episode: 14, duration: 127.134s, episode steps: 902, steps per second:   7, episode reward: 195.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.758 [0.000, 5.000],  loss: 0.298411, mean_q: 6.125110, mean_eps: 0.190945\n",
      "done, took 2284.890 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c9419defc8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps = 10000, visualize = False, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 45.000, steps: 466\n",
      "Episode 2: reward: 60.000, steps: 508\n",
      "Episode 3: reward: 20.000, steps: 433\n",
      "Episode 4: reward: 80.000, steps: 527\n",
      "Episode 5: reward: 385.000, steps: 1115\n",
      "Episode 6: reward: 330.000, steps: 1137\n",
      "Episode 7: reward: 130.000, steps: 542\n",
      "Episode 8: reward: 40.000, steps: 570\n",
      "Episode 9: reward: 395.000, steps: 1619\n",
      "Episode 10: reward: 195.000, steps: 652\n",
      "168.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 10, visualize = True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIP] Next time specify overwrite=True!\n"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('Saved_Weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('Saved_Weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 480.000, steps: 1188\n",
      "Episode 2: reward: 150.000, steps: 642\n",
      "Episode 3: reward: 375.000, steps: 1232\n",
      "Episode 4: reward: 70.000, steps: 570\n",
      "Episode 5: reward: 355.000, steps: 956\n",
      "Episode 6: reward: 185.000, steps: 788\n",
      "Episode 7: reward: 75.000, steps: 561\n",
      "Episode 8: reward: 235.000, steps: 993\n",
      "Episode 9: reward: 135.000, steps: 642\n",
      "Episode 10: reward: 25.000, steps: 540\n",
      "208.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 10, visualize = True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement Learning For Space Invaders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
